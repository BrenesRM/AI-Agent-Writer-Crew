"""
P√°gina de configuraci√≥n del sistema
"""

import streamlit as st
import os
from pathlib import Path
from frontend.utils.session_state import add_log_entry

def render_settings():
    """Renderiza la p√°gina de configuraci√≥n"""
    
    st.title("‚öôÔ∏è Configuraci√≥n del Sistema")
    st.markdown("Configura los par√°metros del sistema y las integraciones.")
    
    # Tabs para organizar configuraciones
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ü§ñ Modelos LLM", 
        "üî§ Embeddings", 
        "üóÑÔ∏è Base de Datos", 
        "üîß Sistema",
        "üìä Logs"
    ])
    
    with tab1:
        render_llm_settings()
        
    with tab2:
        render_embeddings_settings()
        
    with tab3:
        render_database_settings()
        
    with tab4:
        render_system_settings()
        
    with tab5:
        render_logs_viewer()

def render_llm_settings():
    """Configuraci√≥n de modelos LLM"""
    
    st.subheader("ü§ñ Configuraci√≥n de Modelos LLM")
    
    # Selector de proveedor
    provider = st.selectbox(
        "Proveedor de LLM",
        ["local", "openai", "anthropic", "custom"],
        index=0 if st.session_state.llm_provider == "local" else 1,
        help="Selecciona el proveedor de modelos de lenguaje"
    )
    
    st.session_state.llm_provider = provider
    
    if provider == "local":
        render_local_llm_config()
    elif provider == "openai":
        render_openai_config()
    elif provider == "anthropic":
        render_anthropic_config()
    else:
        render_custom_llm_config()

def render_local_llm_config():
    """Configuraci√≥n para LLM local"""
    
    st.markdown("### üè† Configuraci√≥n LLM Local")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Selector de servidor local
        local_server = st.selectbox(
            "Servidor Local",
            ["Ollama", "vLLM", "llama.cpp", "Text Generation WebUI"],
            help="Tipo de servidor local para LLM"
        )
        
        # URL del servidor
        server_url = st.text_input(
            "URL del Servidor",
            value="http://localhost:11434",
            help="URL donde est√° ejecut√°ndose el servidor LLM"
        )
        
        # Modelo seleccionado
        if local_server == "Ollama":
            available_models = get_ollama_models()
            model_name = st.selectbox(
                "Modelo",
                available_models,
                help="Modelo disponible en Ollama"
            )
        else:
            model_name = st.text_input(
                "Nombre del Modelo",
                value="deepseek-r1-0528-qwen3-8b-q4",
                help="Nombre exacto del modelo"
            )
    
    with col2:
        # Par√°metros del modelo
        st.markdown("**Par√°metros:**")
        
        temperature = st.slider(
            "Temperatura",
            min_value=0.0,
            max_value=2.0,
            value=0.7,
            step=0.1,
            help="Creatividad del modelo (0 = determin√≠stico, 2 = muy creativo)"
        )
        
        max_tokens = st.number_input(
            "M√°ximo de Tokens",
            min_value=100,
            max_value=8192,
            value=2048,
            help="M√°ximo n√∫mero de tokens en la respuesta"
        )
        
        top_p = st.slider(
            "Top P",
            min_value=0.0,
            max_value=1.0,
            value=0.9,
            step=0.05,
            help="Nucleus sampling parameter"
        )
    
    # Test de conexi√≥n
    if st.button("üîç Probar Conexi√≥n"):
        test_local_llm_connection(server_url, model_name)
    
    # Informaci√≥n del modelo actual
    if st.session_state.get('current_model_info'):
        st.markdown("### üìä Informaci√≥n del Modelo Actual")
        info = st.session_state.current_model_info
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Modelo", info.get('name', 'N/A'))
        with col2:
            st.metric("Tama√±o", info.get('size', 'N/A'))
        with col3:
            st.metric("Estado", info.get('status', 'N/A'))

def get_ollama_models():
    """Obtiene la lista de modelos disponibles en Ollama"""
    try:
        # Aqu√≠ se conectar√≠a con Ollama para obtener modelos reales
        # Por ahora, devolvemos una lista simulada
        return [
            "deepseek-r1:latest",
            "llama2:7b",
            "llama2:13b",
            "mistral:latest",
            "codellama:latest"
        ]
    except:
        return ["No se pudieron obtener modelos"]

def test_local_llm_connection(server_url: str, model_name: str):
    """Prueba la conexi√≥n con el LLM local"""
    
    with st.spinner("Probando conexi√≥n..."):
        try:
            # Simular test de conexi√≥n
            import time
            time.sleep(2)
            
            # En implementaci√≥n real, aqu√≠ har√≠amos una llamada al servidor
            st.success(f"‚úÖ Conexi√≥n exitosa con {model_name}")
            st.session_state.current_model_info = {
                'name': model_name,
                'size': '4.1GB',
                'status': 'Disponible'
            }
            add_log_entry(f"Conexi√≥n LLM local exitosa: {model_name}", "success", "llm_config")
            
        except Exception as e:
            st.error(f"‚ùå Error de conexi√≥n: {e}")
            add_log_entry(f"Error conexi√≥n LLM: {str(e)}", "error", "llm_config")

def render_openai_config():
    """Configuraci√≥n para OpenAI"""
    
    st.markdown("### ü§ñ Configuraci√≥n OpenAI")
    
    # API Key
    api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        help="Tu clave API de OpenAI"
    )
    
    if api_key:
        os.environ['OPENAI_API_KEY'] = api_key
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Modelo
        model = st.selectbox(
            "Modelo",
            ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"],
            help="Modelo de OpenAI a usar"
        )
        
        # Organizaci√≥n (opcional)
        organization = st.text_input(
            "Organizaci√≥n (opcional)",
            help="ID de organizaci√≥n de OpenAI"
        )
    
    with col2:
        # Par√°metros
        temperature = st.slider("Temperatura", 0.0, 2.0, 0.7, 0.1)
        max_tokens = st.number_input("M√°ximo Tokens", 100, 4000, 2048)
    
    # Test de API
    if api_key and st.button("üîç Probar API"):
        test_openai_connection(api_key, model)

def test_openai_connection(api_key: str, model: str):
    """Prueba la conexi√≥n con OpenAI"""
    
    with st.spinner("Probando API de OpenAI..."):
        try:
            # Simulaci√≥n - en realidad har√≠amos una llamada a OpenAI
            import time
            time.sleep(2)
            st.success(f"‚úÖ Conexi√≥n exitosa con OpenAI {model}")
            add_log_entry(f"Conexi√≥n OpenAI exitosa: {model}", "success", "openai_config")
            
        except Exception as e:
            st.error(f"‚ùå Error con OpenAI API: {e}")
            add_log_entry(f"Error OpenAI: {str(e)}", "error", "openai_config")

def render_anthropic_config():
    """Configuraci√≥n para Anthropic Claude"""
    
    st.markdown("### üß† Configuraci√≥n Anthropic Claude")
    st.info("Configuraci√≥n para Anthropic Claude en desarrollo")

def render_custom_llm_config():
    """Configuraci√≥n para LLM personalizado"""
    
    st.markdown("### üîß Configuraci√≥n LLM Personalizado")
    st.info("Configuraci√≥n para LLM personalizado en desarrollo")

def render_embeddings_settings():
    """Configuraci√≥n de embeddings"""
    
    st.subheader("üî§ Configuraci√≥n de Embeddings")
    
    # Proveedor de embeddings
    embeddings_provider = st.selectbox(
        "Proveedor de Embeddings",
        ["local", "openai", "huggingface"],
        index=0 if st.session_state.embeddings_provider == "local" else 1,
        help="Selecciona el proveedor para generar embeddings"
    )
    
    st