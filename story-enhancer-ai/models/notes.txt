Local LLaMA Agent

This project allows you to run a local LLaMA 3.2 1B Instruct model using Python and llama-cpp-python. You can use it interactively or integrate it into your own agents framework.

?? Project Structure
models/
+-- llama-3.2-1b-instruct-q8_0.gguf   # Your LLaMA model file
+-- test_llama.py                       # Simple test script
+-- agent_example.py                     # Interactive agent example
+-- Modelfile                            # Optional

?? Requirements

Ubuntu (tested on 22.04+)

Python 3.12+

llama-cpp-python library

Virtual environment (recommended)

?? Installation

Update system packages:

sudo apt update
sudo apt install -y build-essential cmake git python3 python3-pip


Create and activate a Python virtual environment:

python3 -m venv ~/llama-venv
source ~/llama-venv/bin/activate


Upgrade pip and install dependencies:

pip install --upgrade pip
pip install llama-cpp-python


Place your .gguf model in the models/ folder:

models/llama-3.2-1b-instruct-q8_0.gguf

?? Usage
Test the model
python3 test_llama.py

Run the interactive agent

Make the script executable:

chmod +x agent_example.py


Run it:

./agent_example.py


Enter a prompt for the agent:

Explain ransomware prevention techniques.


The model will respond locally.

? Tips

Adjust threads in agent_example.py for performance:

llm = Llama(model_path="./llama-3.2-1b-instruct-q8_0.gguf", n_threads=4)


Use a virtual environment to avoid system Python conflicts.

Keep .gguf models centralized in models/ for easy agent integration.

?? References

llama-cpp-python GitHub

LLaMA.cpp repository